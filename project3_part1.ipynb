{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Web APIs & NLP (Part 1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Men In Black (MIB) is an association that researches & promotes extraterrestrial knowledge. MIB sent a request to us, the Reddit data science team, to create a model to classify reddit posts and identify the key words that characterizes the post as \"Aliens\" from \"Space\". This would facilitate MIB's intelligence collection efforts about sentiments from ordinary citizens and promote its marketing effort on social media, events and podcasts.\n",
    "  \n",
    "The goal of the project is to create an accurate classification model and discover the key words that best differentiates posts related to \"Aliens\" from \"Space\" in reddit.\n",
    "  \n",
    "Three classification models, Naive Bayes, Logistic Regression and Random Forest will be developed to assist with the problem statement.\n",
    "  \n",
    "The success of the model will be assessed based on its accuracy and F1 score on unseen test data.\n",
    "\n",
    "The primary stakeholder is Man In Black since we are addressing their request to seggregate aliens posts from space posts for marketing purposes. Secondary stakeholders such as UFO or Space enthutiasts would also benefit from such analysis because they can pay more attention to top words used in their respective discussion groups, to make sure they understand the hot topics that are being discussed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to TA\n",
    "\n",
    " - Is it clear what the goal of the project is?  \n",
    " Ans: Goal is to create accurate classification model and discover the key words best differentiates posts from \"Aliens\" and \"Space\". \n",
    " - What type of model will be developed?  \n",
    " Ans: Naives Bayes, Logistic Regression and Random Forest. \n",
    " - How will success be evaluated?  \n",
    " Ans: Based on accuracy and F1 score on unseen data. \n",
    " - Is the scope of the project appropriate?  \n",
    " Ans: The scope is limited to posts from \"Aliens\" and \"Space\" as stated in problem statement. \n",
    " - Is it clear who cares about this or why this is important to investigate?  \n",
    " Ans: Men In Black association to promote marketing efforts. \n",
    " - Does the student consider the audience and the primary and secondary stakeholders?  \n",
    " Ans: Main stakeholder is MIB and secondary stakeholders are UFO and Space enthutiasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from pmaw import PushshiftAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrap the first 100 posts from \"Aliens\" subreddit\n",
    "#Create a list for alients posts\n",
    "list_aliens = []\n",
    "url_aliens = 'https://api.pushshift.io/reddit/search/submission'\n",
    "#Set the UTC so that each time web scrapping is re-run the posts extracted are the same \n",
    "params = {\n",
    "    \"subreddit\":'aliens',\n",
    "    \"size\":100,\n",
    "    \"before\":1641624113\n",
    "}\n",
    "#save the posts into list created\n",
    "res_aliens = requests.get(url_aliens, params)\n",
    "data_aliens = res_aliens.json()\n",
    "data_aliens = data_aliens[\"data\"]\n",
    "list_aliens.extend(data_aliens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrap the next 2000 posts from \"Aliens\" subreddit using loop\n",
    "# set num to be the last post from previous 100 extracted posts so that the UTC is not duplicated\n",
    "num = 99\n",
    "for i in range(20):\n",
    "    url_aliens = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    params = {\n",
    "    \"subreddit\":'aliens',\n",
    "    \"size\":100,\n",
    "    'before': list_aliens[num]['created_utc']\n",
    "}\n",
    "    res_aliens = requests.get(url_aliens, params)\n",
    "    data_aliens = res_aliens.json()\n",
    "    data_aliens = data_aliens[\"data\"]\n",
    "    list_aliens.extend(data_aliens)\n",
    "    num +=100\n",
    "#give 3 seconds \"rest\" time to the server receiving the requests to avoid being suspicious or crashing the webpage\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_aliens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list_aliens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100, 82)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_is_blocked</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>...</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>media_metadata</th>\n",
       "      <th>is_gallery</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>edited</th>\n",
       "      <th>banned_by</th>\n",
       "      <th>gilded</th>\n",
       "      <th>gallery_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>TV_quirks</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_h1gmxqqf</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>Outrageous_Resist447</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_daj28e5r</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>Otherwise-Book-5327</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_8uxu5a93</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>SilentConsciou5</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_cdee3vc1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>gamerjam</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_14i9x9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  allow_live_comments                author  \\\n",
       "0            []                False             TV_quirks   \n",
       "1            []                False  Outrageous_Resist447   \n",
       "2            []                False   Otherwise-Book-5327   \n",
       "3            []                False       SilentConsciou5   \n",
       "4            []                False              gamerjam   \n",
       "\n",
       "  author_flair_css_class author_flair_richtext author_flair_text  \\\n",
       "0                   None                    []              None   \n",
       "1                   None                    []              None   \n",
       "2                   None                    []              None   \n",
       "3                   None                    []              None   \n",
       "4                   None                    []              None   \n",
       "\n",
       "  author_flair_type author_fullname  author_is_blocked author_patreon_flair  \\\n",
       "0              text     t2_h1gmxqqf              False                False   \n",
       "1              text     t2_daj28e5r              False                False   \n",
       "2              text     t2_8uxu5a93              False                False   \n",
       "3              text     t2_cdee3vc1              False                False   \n",
       "4              text       t2_14i9x9              False                False   \n",
       "\n",
       "   ... author_flair_template_id author_flair_text_color  media_metadata  \\\n",
       "0  ...                      NaN                     NaN             NaN   \n",
       "1  ...                      NaN                     NaN             NaN   \n",
       "2  ...                      NaN                     NaN             NaN   \n",
       "3  ...                      NaN                     NaN             NaN   \n",
       "4  ...                      NaN                     NaN             NaN   \n",
       "\n",
       "   is_gallery  author_cakeday author_flair_background_color edited banned_by  \\\n",
       "0         NaN             NaN                           NaN    NaN       NaN   \n",
       "1         NaN             NaN                           NaN    NaN       NaN   \n",
       "2         NaN             NaN                           NaN    NaN       NaN   \n",
       "3         NaN             NaN                           NaN    NaN       NaN   \n",
       "4         NaN             NaN                           NaN    NaN       NaN   \n",
       "\n",
       "  gilded  gallery_data  \n",
       "0    NaN           NaN  \n",
       "1    NaN           NaN  \n",
       "2    NaN           NaN  \n",
       "3    NaN           NaN  \n",
       "4    NaN           NaN  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('aliens_raw',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrap the first 100 posts from the other subreddit - \"Space\"\n",
    "list_space = []\n",
    "url_space = 'https://api.pushshift.io/reddit/search/submission'\n",
    "params = {\n",
    "    \"subreddit\":'space',\n",
    "    \"size\":100,\n",
    "#Set the UTC so that each time web scrapping is re-run the posts extracted are the same \n",
    "    \"before\":1641624113\n",
    "}\n",
    "#save the posts into list created\n",
    "res_space = requests.get(url_space, params)\n",
    "data_space = res_space.json()\n",
    "data_space = data_space[\"data\"]\n",
    "list_space.extend(data_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrap the next 2000 posts from \"Aliens\" subreddit using loop\n",
    "# set num to be the last post from previous 100 extracted posts so that the UTC is not duplicated\n",
    "num = 99\n",
    "for i in range(20):\n",
    "    url_space = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    params = {\n",
    "    \"subreddit\":'space',\n",
    "    \"size\":100,\n",
    "    'before': list_space[num]['created_utc']\n",
    "}\n",
    "    res_space = requests.get(url_space, params)\n",
    "    data_space = res_space.json()\n",
    "    data_space = data_space[\"data\"]\n",
    "    list_space.extend(data_space)\n",
    "    num +=100\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_space = pd.DataFrame(list_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_space.to_csv(\"space_raw\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note for TA  \n",
    " - Was enough data gathered to generate a significant result?  \n",
    "Ans: scrapped 2100 for each posts  \n",
    " - Was data collected that was useful and relevant to the project?   \n",
    "Ans: only collected posts from the 2 subreddits as stated in problem statement - aliens and space\n",
    " - Was data collection and storage optimized through custom functions, pipelines, and/or automation?   \n",
    "Ans: used for loop to scrapped the remaining 2000 posts from each subreddits\n",
    " - Was thought given to the server receiving the requests such as considering number of requests per second?      \n",
    "Ans: Gave 3 seconds for each 100 posts extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First part of the project - collecting posts from 2 subreddits, is completed. Remaining work done in Notebook part 2. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
